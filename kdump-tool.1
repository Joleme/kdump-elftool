.TH kdump-tool 1 06/02/01  "Kdump dump handling tool"

.SH NAME
kdump-tool \- Kdump dump handling tool

.SH SYNOPSIS
.B kdump-tool
[\-\-help] topelf [\--help]
[\-\-oldmem|\-i <oldmem>]
[\-\-outfile|-o <output file>]
[\-\-vmcore|-v <vmcore>]
[\-\-elfclass|-c 32|64]
[\-\-level|-l all|inuse|user|cache|kernel]
[\-\-extravminfo|-e <vminfofile>]
[\-\-debug|-d]

.B kdump-tool
[\-\-help] tovelf [\--help]
[\-\-infile|\-i <oldmem or pelf>]
[\-\-outfile|-o <output file>]
[\-\-vmcore|-v <vmcore>]
[\-\-intype|-I oldmem|pelf]
[\-\-physpgd|-P <pgd phys address>]
[\-\-elfclass|-c 32|64]
[\-\-level|-l all|inuse|user|cache|kernel]
[\-\-vmlinux|-m <vmlinux>]
[\-\-extravminfo|-e <vminfofile>]
[\-\-procthreads|-p]
[\-\-debug|-d]

.B kdump-tool
[\-\-help] addrandoff [\--help]
[\-\-vmcore|-v <vmcore>]
[\-\-vmlinux|-m <vmlinux>]
[\-\-extravminfo|-e <vminfofile>]

.B kdump-tool
[\-\-help] makedyn [\--help]
<vmlinux> [<vmlinux> [...]]

.B kdump-tool
[\-\-help] dumpmem [\--help]
[\-\-infile|\-i <oldmem or pelf>]
[\-\-outfile|-o <output file>]
[\-\-vmcore|-v <vmcore>]
[\-\-intype|-I oldmem|pelf]
[\-\-is_physical|-p]
[\-\-extravminfo|-e <vminfofile>]
<address> <size>

.B kdump-tool
[\-\-help] virttophys [\--help]
[\-\-infile|\-i <oldmem or pelf>]
[\-\-vmcore|-v <vmcore>]
[\-\-intype|-I oldmem|pelf]
[\-\-physpgd|-P <pgd phys address>]
[\-\-extravminfo|-e <vminfofile>]
[\-\-debug|-d]
<address>

.SH DESCRIPTION
The
.BR kdump-tool
program extracts kernel coredumps from the target and processes those
coredumps into something gdb can understand.
.PP
In a kdump crash kernel, there are two files that let you extract
information about the kernel that just crashed: /dev/mem and
/proc/vmcore.  /dev/mem is a raw file holding an image of the physical
memory of the system, including the previous host's memory.
/proc/vmcore is a kernel coredump plus some extra information.  (See
.BR RATONALE
for why this coredump is not sufficient.)

.BR kdump-tool
deals with two other types of files, both elf files.  Physical memory
elf files (pelf) and virtual memory elf files (velf).  A pelf file
holds a physical memory coredump, basically what comes out of
/dev/mem in elf format.  Note that
.BR kdump-tool
will get the physical memory ranges for the old kernel out of
/proc/vmcore, if available, and only store the necessary ranges.

A velf file holds a virtual memory coredump, this is in a format that
gdb can load load and use.  gdb needs the virtual kernel addresses to
work.

.BR "kdump-tool topelf -o <pelf file>"
will process /dev/mem and /proc/vmcore into a pelf file.  This is
generally preferred on a target system, pelf files are usually smaller
than velf files and they are quicker to process.  Plus a velf file may
not have all of physical memory, in case you are in a desperate
situation and need to get to userland memory.  Note that the output
will go to standard output if you don't specify -o.

.BR "kdump-tool tovelf -i <pelf file> -o <velf file>"
will process the pelf file into a velf file.

.BR "kdump-tool tovelf -I outmem -o <velf file>"
will process /dev/mem and /dev/vmcore directly into a velf file.

For
.BR gdb
to properly interpret kernels with a random base address, it has to be
told the load offset.  It has to have the original
.BR <vmlinux>
file to do this, then it can compare symbol offsets to symbol addresses
it has access to in the dump file.  Adding the
.BR "--vmlinux <vmlinux>"
option to the
.BR tovelf
subcommand will do this, or you can add it later to an existing virtual
memory coredump with the
.BR "kdump-tool addrandoff -m <vmlinux> -v <vmcore>"
command.

As an additional problem for debugging kernels with a random base address,
the
.BR vmlinux
compile for most architectures is marked as a regular executable, but
.BR gdb
requires that it be a relocatable executable to handle the load offset.
The command
.BR "kdump-tool makedyn <vmlinux>"
will convert a vmlinux to a relocatable executable command for you.

.BR kdump-tool
will also dump ranges of memory from the various input types it
supports using the
.BR dumpmem
subcommand.  The parameters all work as
before, with the addition of -p, which specifies the address as
physical.  Otherwise the address is assumed to be virtual.

.BR kdump-tool
can also convert logical addresses to physical addresses using the
.BR virttophys
subcommand.  This only works on physical memory files.

.SH OPTIONS
.TP
.I "\-\-help"
Output help.
.TP
.I "\-\-oldmem|\-i <oldmem>"
Set the location of the oldmem file.  The default -s /dev/mem.
.TP
.I "\-\-outfile|-o <output file>"
Set the output file, output will go to stdout if this is not supplied.
.TP
.I "\-\-vmcore|-v <vmcore>"
Set the location of the vmcore file, /proc/vmcore by default.
.TP
.I "\-\-vmlinux|-m <vmlinux>"
The vmlinux file for the core dump being processed.  Require to add the
load offset for a randomized base kernel.
.TP
.I "\-\-infile|\-i <oldmem or pelf>"
Set the location of the input file for converting to velf.  This can
either be a raw memory file (/dev/mem by default) if
.BR \-I outmem
is specified.  If
.BR \-I outmem
is not given, then this parameter is required.
.TP
.I "\-\-intype|-I outmem|pelf"
Set the input file time for conversion to velf.
.TP
.I "\-\-physpgd|-P <pgd phys address>"
Set the physical address of the pgd pointer.  This is read from the
input file, but if it is missing or wrong it can be overridden.  This
can also be used to create a coredump of a process if you have the
process' physical page directory address.  That can be obtained with
gdb.  It will only dump the process memory that is present, so if
memory is not yet paged in or has been paged out, it will not be
present.
.TP
.I "\-\-elfclass|-c 32|64"
All physical memory coredump are 64-bit because many 32-bit
architectures can map physical addresses greater than 32-bits (like
x86 PAE).  For architectures, like MIPS, where the kernel may be 32 or
64 bits, you have to set the class to 32 bits if you have a 32-bit
kernel (or are generating a coredump for a userland process that is
32-bits).  x86_64 and i386 are distinct architectures, so you won't
have a problem with the kernel, but you might have the same issue for
userland.
.TP
.I "\-\-level|-l all|inuse|user|cache|kernel"
Select which pages of memory to put into the dump.  The default level
is kernel, which will dump pages used by the kernel.  This is the most
useful, in general, for coredump analysis since free, cache, and user
pages aren't terribly relevant to analyzing the kernel.  The cache
level will dump kernel and cache pages.  The user level will dump
kernel and user pages.  The inuse level will dump all pages that are
not free, and all obviously just dumps all pages.
.TP
.I "\-\-extravminfo|-e <vminfofile>"
Add extra symbol and offset information that was not available in the
kernel dump.  This will override existing information in the kernel
dump if there are duplicates.
.TP
.I "\-\-procthreads|-p"
Convert processed in the kernel into threads that gdb can understand.
See "PROCESSES TO GDB THREADS" below for more details.
.TP
.I "\-\-debug|-d"
Dump debug information.  Generally only useful if you are debugging
kdump-tool itself.

.SH RATIONALE
Why not just use /proc/vmcore, you ask?  The /proc/vmcore file
generated by the kernel does not have all the virtual memory sections
available.  Particularly, vmalloc memory and vmemmap are not
available, and all of physical memory may not be present.  Since
modules reside in vmalloc and some systems have the pages array in
vmemmap, it's almost impossible to use gdb on the standard kernel
coredump.

.SH PROCESSES TO GDB THREADS
kdump-tool can convert every kernel process into a thread that gdb
can use.  You generally have to use the macro from kdump_gdbinit
named thread_vminfo to get the extra vminfo file, pass it in to
kdump-tool with
.I \-\-extravminfo <file>
with that file to get the proper symbols.  Then use the
.I \-\-procthreads
option to do the conversion.  The procedure would generally be

.RS 4
Get a physical dump of the kernel.  Physical dumps are generally recommended
for the target, they are smaller and can be easily processed on the host.

<Convert it to a normal virtual dump.  This is required because if the
 kernel is relocated, you need relocated symbols.>

kdump-tool tovelf -I pelf -i pdump -m vmlinux -o vdump

gdb vmlinux vdump

source kdump_gdbinit

thread_vminfo_<arch>  # <arch> is either mips or x86_64

<save the output to a file name thread_vminfo and quit gdb>

kdump-tool tovelf -I pelf -i pdump -m vmlinux -o vdump -e thread_vminfo -p

gdb vmlinux vdump
.RE

And you should see all the process as threads.

Note that you do
.I NOT
get the userland traceback here.  You get the
kernel side of the traceback.

.SH PROCESSES TO GDB THREADS EXTRAS FOR X86
Unfortuantely, x86 doesn't provide all the information you need to
properly generate threads for each process.  Two additional pieces
of information are required: The context switch point and the value
of the BP register at context switch.

The kernel does not store the value of the BP register at context
switch, and that would slow down context switches a little so it's
frowned upon.  So to get this, you must calculate the frame size
of __schedule and set that in thread_vminfo file.

To calculate it, load the vmlinux file into gdb (as before) and do

.RS 4
x/20i __schedule
.RE

You should see something like:

.RS 4
0xffffffff81744290 <__schedule>:	push   %rbp
0xffffffff81744291 <__schedule+1>:	mov    $0xdf40,%rax
0xffffffff81744298 <__schedule+8>:	mov    %gs:0x9908,%rdx
0xffffffff817442a1 <__schedule+17>:	mov    %rsp,%rbp
0xffffffff817442a4 <__schedule+20>:	push   %r15
0xffffffff817442a6 <__schedule+22>:	push   %r14
0xffffffff817442a8 <__schedule+24>:	push   %r13
0xffffffff817442aa <__schedule+26>:	push   %r12
0xffffffff817442ac <__schedule+28>:	push   %rbx
0xffffffff817442ad <__schedule+29>:	sub    $0x48,%rsp
.RE

You need to count how much the stack is decremented here after the
push of %rbp.  On x86_64, each push is 8 bytes, then you see a direct
subtraction from %rsp.  So there are five pushes and then another 72
(0x48) bytes subtracted, so this is 72 + (5 * 8), or 112 bytes.  Then
set the

.RS 4
SIZE(context_switch_frame)=1
.RE

in your thread_vminfo file from one to the value you calculate.

If you have the x86 patch in the kernel-patches directory applied to
your kernel, then it should contain the code to add the context switch
point.  If you don't then all is not lost, you can find it.  First
convert your physical coredump into a normal virtual coredump and load
it in to gdb.  (You have to actually do this with a coredump because
the kernel can be relocated and you need to know the relocated symbol
point.)  Then do:

.RS 4
x/10i __schedule
.RE

Just keep hitting return until you find a call to __switch_to, like:

.RS 4
0xffffffff81744513 <__schedule+643>:	mov    %rsp,0x408(%rdi)
0xffffffff8174451a <__schedule+650>:	mov    0x408(%rsi),%rsp
0xffffffff81744521 <__schedule+657>:	callq  0xffffffff810013c0 <__switch_to>
0xffffffff81744526 <__schedule+662>:	mov    %gs:0x9900,%rsi
.RE

Get the location of the instruction right before the callq and add:

.RS 4
SYMBOL(__thread_sleep_point)=ffffffff8174451a
.RE

to your thread_vminfo file.

.SH FILES
/dev/mem, /proc/vmcore

See Documentation/kdump/kdump.txt in the Linux kernel for more details.

.SH "SEE ALSO"
kexec(8)

.SH "KNOWN PROBLEMS"
This is still fairly primitive and doesn't support all architectures.

Process to gdb thread processing only works for MIPS and X86_64.

.SH AUTHOR
.PP
Corey Minyard <minyard@acm.org>
